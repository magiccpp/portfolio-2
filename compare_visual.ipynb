{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare model and human brain\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the date must be working day!\n",
    "model_trace_log = [\n",
    "  {\n",
    "    \"date\": \"2024-05-15\",\n",
    "    \"weights\": [\n",
    "      {\n",
    "        \"id\": \"STAN.L\",\n",
    "        \"weight\": 0.0988379414494704\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"KO\",\n",
    "        \"weight\": 0.10885514872237066\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LLY\",\n",
    "        \"weight\": 0.16274045692599903\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"GILD\",\n",
    "        \"weight\": 0.0982685011890021\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"MRK\",\n",
    "        \"weight\": 0.09608726469000152\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"NOW\",\n",
    "        \"weight\": 0.10724631420200852\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"VRTX\",\n",
    "        \"weight\": 0.14695857799430817\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"WMT\",\n",
    "        \"weight\": 0.18100579482683965\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-05-16\",\n",
    "    \"weights\": [\n",
    "      {\n",
    "        \"id\": \"CHD\",\n",
    "        \"weight\": 0.09604227989932394\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"LLY\",\n",
    "        \"weight\": 0.15701155301162809\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"GILD\",\n",
    "        \"weight\": 0.08309850279939462\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"MCK\",\n",
    "        \"weight\": 0.15491295395567367\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"MRK\",\n",
    "        \"weight\": 0.12300274846059159\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"NOW\",\n",
    "        \"weight\": 0.0998352785021135\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"VRTX\",\n",
    "        \"weight\": 0.10564614375465124\n",
    "      },\n",
    "      {\n",
    "        \"id\": \"WMT\",\n",
    "        \"weight\": 0.18045053961662333\n",
    "      }\n",
    "    ]\n",
    "  }, \n",
    "  {\n",
    "    \"date\": \"2024-05-22\",\n",
    "    \"weights\": [\n",
    "        {\n",
    "          \"id\": \"KO\",\n",
    "          \"weight\": 0.10274849558771942 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"LLY\",\n",
    "          \"weight\": 0.1566002200958428 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"GILD\",\n",
    "          \"weight\": 0.08707934623088542\n",
    "        },\n",
    "        {\n",
    "          \"id\": \"J\",\n",
    "          \"weight\": 0.11359767227893916 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"MRK\",\n",
    "          \"weight\": 0.10567158852690067 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"NOW\",\n",
    "          \"weight\": 0.11206722758523373 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"VRTX\",\n",
    "          \"weight\": 0.142924247158989 \n",
    "        },\n",
    "        {\n",
    "          \"id\": \"WMT\",\n",
    "          \"weight\": 0.17931120253548977 \n",
    "        }\n",
    "      ]\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-06-10\",\n",
    "    \"weights\": [\n",
    "        {\n",
    "            \"id\": \"STAN.L\",\n",
    "            \"weight\": 0.099809488530\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"LLY\",\n",
    "            \"weight\": 0.141107090394177\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"GILD\",\n",
    "            \"weight\": 0.14317362063347\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"JKHY\",\n",
    "            \"weight\": 0.13209880165885\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"MCK\",\n",
    "            \"weight\": 0.098208917045372\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"NVR\",\n",
    "            \"weight\": 0.177313820149842\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"NOW\",\n",
    "            \"weight\": 0.087160539294239\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"VRTX\",\n",
    "            \"weight\": 0.12112772229333\n",
    "        }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-08-08\",\n",
    "    \"weights\": [\n",
    "    {\n",
    "        \"id\": \"STAN.L\",\n",
    "        \"weight\": 0.06940932033704353\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"AMGN\",\n",
    "        \"weight\": 0.07431243214457392\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"BIIB\",\n",
    "        \"weight\": 0.042900139778976665\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"COST\",\n",
    "        \"weight\": 0.03558101407191769\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"CTRA\",\n",
    "        \"weight\": 0.05635014005671591\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"LLY\",\n",
    "        \"weight\": 0.03948522028075085\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"GILD\",\n",
    "        \"weight\": 0.1454264331821711\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"HOLX\",\n",
    "        \"weight\": 0.05758084586182881\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"JBL\",\n",
    "        \"weight\": 0.005493340409292151\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"JKHY\",\n",
    "        \"weight\": 0.1526600094065125\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"MCK\",\n",
    "        \"weight\": 0.07232195285376533\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"NVR\",\n",
    "        \"weight\": 0.06949839934412294\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"PGR\",\n",
    "        \"weight\": 0.04417625540632342\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"NOW\",\n",
    "        \"weight\": 0.05764335559940242\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"STE\",\n",
    "        \"weight\": 0.0076256697455101465\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"SMCI\",\n",
    "        \"weight\": 0.042851698736872286\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TTWO\",\n",
    "        \"weight\": 0.010628803276210979\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"VRTX\",\n",
    "        \"weight\": 0.01605496950800943\n",
    "    }\n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-08-19\",\n",
    "    \"weights\":  [\n",
    "      {\n",
    "          \"id\": \"STAN.L\",\n",
    "          \"weight\": 0.02090530701681672\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"CTRA\",\n",
    "          \"weight\": 0.039525249097928664\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"LLY\",\n",
    "          \"weight\": 0.07781991214048126\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"GILD\",\n",
    "          \"weight\": 0.103091631334796\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"JKHY\",\n",
    "          \"weight\": 0.19322764738301704\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"NVR\",\n",
    "          \"weight\": 0.19999999999999962\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"SMCI\",\n",
    "          \"weight\": 0.06095072592843817\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"UNH\",\n",
    "          \"weight\": 0.0997655409399469\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"VRTX\",\n",
    "          \"weight\": 0.02215266539408572\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"WMT\",\n",
    "          \"weight\": 0.07527618703969421\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"600276.SS\",\n",
    "          \"weight\": 0.02574741655617227\n",
    "      },\n",
    "      {\n",
    "          \"id\": \"601225.SS\",\n",
    "          \"weight\": 0.08108564359822486\n",
    "      }\n",
    "  ]\n",
    "  },\n",
    "  {\n",
    "      \"date\": \"2024-08-27\",\n",
    "      \"weights\":   [\n",
    "        {\n",
    "            \"id\": \"RHM.DE\",\n",
    "            \"weight\": 0.011395852207041484\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"FRAS.L\",\n",
    "            \"weight\": 0.09640906381745395\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"CTRA\",\n",
    "            \"weight\": 0.03023819619758223\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"LLY\",\n",
    "            \"weight\": 0.058479654890121\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"GILD\",\n",
    "            \"weight\": 0.16904811360896962\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"JKHY\",\n",
    "            \"weight\": 0.2\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"PGR\",\n",
    "            \"weight\": 0.06012139904756649\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"SMCI\",\n",
    "            \"weight\": 0.07466780405733559\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"VRTX\",\n",
    "            \"weight\": 0.026939136514627482\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"WMT\",\n",
    "            \"weight\": 0.09133278380032214\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"600150.SS\",\n",
    "            \"weight\": 0.018772858604274755\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"600519.SS\",\n",
    "            \"weight\": 0.12040575367173317\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"600900.SS\",\n",
    "            \"weight\": 0.0014987798535868154\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"601225.SS\",\n",
    "            \"weight\": 0.04069060372937267\n",
    "        }\n",
    "    ]}\n",
    "]\n",
    "\n",
    "human_trace_log = [{\n",
    "  \"date\": \"2024-05-15\",\n",
    "  \"weights\": [\n",
    "    {\n",
    "      \"id\": \"ARM\",\n",
    "      \"weight\": 0.2\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"INTC\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"MSFT\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"NVDA\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"SAAB-B.ST\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"RHM.DE\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"BA.L\",\n",
    "      \"weight\": 0.1\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"MU\",\n",
    "      \"weight\": 0.1\n",
    "    }\n",
    "  ]\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique stock ids and determine the overall period for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_data(trace_log_dir, trace_log):\n",
    "  folder = trace_log_dir\n",
    "  # the list of the json files\n",
    "  files = os.listdir(folder)\n",
    "  # sort the files\n",
    "  files.sort()\n",
    "\n",
    "  for file in files:\n",
    "      # get the date from the file name\n",
    "      date = file.split('.')[0].split('_')[-1]\n",
    "      # convert the date from YYYYMMDD to YYYY-MM-DD\n",
    "      date = '-'.join([date[:4], date[4:6], date[6:]])\n",
    "      print('loading data for', date) \n",
    "      # read the json\n",
    "      with open(os.path.join(folder, file)) as f:\n",
    "          data = json.load(f)\n",
    "      \n",
    "      if '_' in data[0][\"id\"]:\n",
    "            # Dictionary to store combined weights\n",
    "            combined_weights = {}\n",
    "            for stock in data:\n",
    "                stock_id = stock[\"id\"].split('_')[0]\n",
    "                weight = stock[\"weight\"]\n",
    "                if stock_id in combined_weights:\n",
    "                    combined_weights[stock_id] += weight\n",
    "                else:\n",
    "                    combined_weights[stock_id] = weight\n",
    "            # Convert the dictionary back to a list of dictionaries\n",
    "            data = [{\"id\": stock_id, \"weight\": weight} for stock_id, weight in combined_weights.items()]\n",
    "\n",
    "\n",
    "      # extract the weights\n",
    "      trace_log.append(\n",
    "          {\n",
    "              \"date\": date,\n",
    "              \"weights\": data\n",
    "          }\n",
    "      )\n",
    "  return trace_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate the folder: processed_data_128/computed_portfolios, read the json files\n",
    "# and extract the weights\n",
    "import os\n",
    "import json\n",
    "# the folder containing the computed portfolios\n",
    "folder = 'processed_data_128/computed_portfolios'\n",
    "# the list of the json files\n",
    "model_trace_log = load_data(folder, model_trace_log)\n",
    "model_trace_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_horizon_trace_log = []\n",
    "model_multi_horizon_trace_log = load_data('multi_horizon', model_multi_horizon_trace_log)\n",
    "print('model_multi_horizon_trace_log:')\n",
    "model_multi_horizon_trace_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = set()\n",
    "for log in model_trace_log:\n",
    "    for asset in log['weights']:\n",
    "        unique_ids.add(asset['id'])\n",
    "\n",
    "for log in human_trace_log:\n",
    "    for asset in log['weights']:\n",
    "        unique_ids.add(asset['id'])\n",
    "\n",
    "for log in model_multi_horizon_trace_log:\n",
    "    for asset in log['weights']:\n",
    "        unique_ids.add(asset['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids.add('^GSPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yfin\n",
    "\n",
    "yfin.pdr_override() \n",
    "\n",
    "stocks_data = pdr.get_data_yahoo(list(unique_ids), start='2024-05-14', end=None)['Adj Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data['^GSPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to USD\n",
    "from util import convert, get_currency_pair\n",
    "\n",
    "\n",
    "for stock_name in list(unique_ids):\n",
    "  stock_suffix = '.' + stock_name.split('.')[-1]\n",
    "  \n",
    "  exchange_name, needs_inversion, exchange_name_yahoo = get_currency_pair(stock_suffix, 'USD')\n",
    "  print(stock_suffix)\n",
    "  print(exchange_name)\n",
    "  if exchange_name is not None:\n",
    "    print(f'Converting {stock_name} to USD')\n",
    "    df = stocks_data[[stock_name]]\n",
    "    df = df.rename(columns={stock_name: 'Adj Close'})\n",
    "    df['Volume'] = 0\n",
    "    df = convert(df, exchange_name, needs_inversion, exchange_name_yahoo)\n",
    "    stocks_data[stock_name] = df['Adj Close']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming stocks_data is already defined and imported\n",
    "# Create a range of dates from the minimum to the maximum date in the original DataFrame\n",
    "all_dates = pd.date_range(start=stocks_data.index.min(), end=stocks_data.index.max(), freq='D')\n",
    "\n",
    "\n",
    "stocks_data = stocks_data.reindex(all_dates, method='ffill')\n",
    "stocks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.fillna(method='ffill', inplace=True)\n",
    "sp500_data = stocks_data['^GSPC']\n",
    "sp500_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ^GSPC from stocks_data\n",
    "stocks_data.drop(columns=['^GSPC'], inplace=True)\n",
    "\n",
    "# remove ^GSPC from unique_ids\n",
    "unique_ids.remove('^GSPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sp500_log_return = pd.DataFrame(np.log(sp500_data)).diff()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_return = pd.DataFrame(np.log(stocks_data)).diff()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return = log_return.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(unique_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(model_trace_log[0]['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(weights_attr, unique_ids):\n",
    "  weights = np.zeros(len(unique_ids))\n",
    "  for asset in weights_attr:\n",
    "    asset_id = asset['id']\n",
    "    asset_weight = asset['weight']\n",
    "    asset_index = sorted(unique_ids).index(asset_id)\n",
    "    weights[asset_index] = asset_weight\n",
    "  return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trace_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi_horizon_trace_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate the dates from start_date to now in the log_return's index\n",
    "\n",
    "cur_asset_model = 1\n",
    "cur_asset_model_hist = [1]\n",
    "\n",
    "cur_asset_model_v2 = 1\n",
    "cur_asset_model_v2_hist = [1]\n",
    "\n",
    "cur_asset_human = 1\n",
    "cur_asset_human_hist = [1]\n",
    "\n",
    "cur_asset_sp500 = 1\n",
    "cur_asset_sp500_hist = [1]\n",
    "\n",
    "model_contrib_hist = {stock: [] for stock in log_return.columns}\n",
    "model_v2_contrib_hist = {stock: [] for stock in log_return.columns}\n",
    "\n",
    "human_contrib_hist = {stock: [] for stock in log_return.columns}\n",
    "sp500_contrib_hist = {stock: [] for stock in log_return.columns}\n",
    "\n",
    "model_weights = np.zeros(len(unique_ids))\n",
    "model_v2_weights = np.zeros(len(unique_ids))\n",
    "human_weights = np.zeros(len(unique_ids))\n",
    "for date in log_return.index:\n",
    "  # get the date as a string\n",
    "  date_str = str(date.date())\n",
    "  # enumerate the trade log to see if the date is in the model_trace_log\n",
    "  for log in model_trace_log:\n",
    "    if log['date'] == date_str:\n",
    "      model_weights = update_weight(log['weights'], unique_ids)\n",
    "\n",
    "  for log in human_trace_log:\n",
    "    if log['date'] == date_str:\n",
    "      human_weights = update_weight(log['weights'], unique_ids)\n",
    "  \n",
    "  daily_log_return_values = log_return.loc[date].values\n",
    "\n",
    "  # get the daily profit\n",
    "  daily_log_return_model = np.dot(model_weights, daily_log_return_values)\n",
    "  daily_model_contributions = model_weights * daily_log_return_values\n",
    "  for stock, contrib in zip(log_return.columns, daily_model_contributions):\n",
    "      model_contrib_hist[stock].append(contrib)\n",
    "\n",
    "  # daily profit for new model are same before 2024-10-10 as the old model\n",
    "  if date < pd.Timestamp('2024-10-10'):\n",
    "    model_v2_weights = model_weights\n",
    "  else:\n",
    "    for log in model_multi_horizon_trace_log:\n",
    "      if log['date'] == date_str:\n",
    "        print(f'update model_v2_weights for {date_str}')\n",
    "        model_v2_weights = update_weight(log['weights'], unique_ids)\n",
    "  \n",
    "  daily_log_return_model_v2 = np.dot(model_v2_weights, daily_log_return_values)\n",
    "  daily_model_v2_contributions = model_v2_weights * daily_log_return_values\n",
    "  for stock, contrib in zip(log_return.columns, daily_model_v2_contributions):\n",
    "    model_v2_contrib_hist[stock].append(contrib)\n",
    "\n",
    "\n",
    "  cur_asset_model = cur_asset_model * np.exp(daily_log_return_model)\n",
    "  \n",
    "  cur_asset_model_hist.append(cur_asset_model)\n",
    "\n",
    "  cur_asset_model_v2 = cur_asset_model_v2 * np.exp(daily_log_return_model_v2)\n",
    "  cur_asset_model_v2_hist.append(cur_asset_model_v2)\n",
    "\n",
    "  daily_log_return_human = np.dot(human_weights, daily_log_return_values)\n",
    "  daily_human_contributions = human_weights * daily_log_return_values\n",
    "  for stock, contrib in zip(log_return.columns, daily_human_contributions):\n",
    "      human_contrib_hist[stock].append(contrib)\n",
    "\n",
    "  cur_asset_human = cur_asset_human * np.exp(daily_log_return_human)\n",
    "  cur_asset_human_hist.append(cur_asset_human)\n",
    "\n",
    "  # if the date is in the human_brain_trace\n",
    "  daily_log_return_sp500 = sp500_log_return.loc[date].values[0]\n",
    "  cur_asset_sp500 = cur_asset_sp500 * np.exp(daily_log_return_sp500)\n",
    "  cur_asset_sp500_hist.append(cur_asset_sp500)\n",
    "\n",
    "\n",
    "# After processing, convert contribution histories to dataframe for easy analysis\n",
    "model_contrib_df = pd.DataFrame(model_contrib_hist, index=log_return.index)\n",
    "model_v2_contrib_df = pd.DataFrame(model_v2_contrib_hist, index=log_return.index)\n",
    "human_contrib_df = pd.DataFrame(human_contrib_hist, index=log_return.index)\n",
    "# Calculate sum of contributions\n",
    "total_model_contrib = model_contrib_df.sum()\n",
    "total_human_contrib = human_contrib_df.sum()\n",
    "\n",
    "# Get the top 5 contributors\n",
    "top5_model_contrib = total_model_contrib.sort_values(ascending=False).head(5)\n",
    "top5_human_contrib = total_human_contrib.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Get the bottom 5 contributors\n",
    "bottom5_model_contrib = total_model_contrib.sort_values().head(5)\n",
    "bottom5_human_contrib = total_human_contrib.sort_values().head(5)\n",
    "print('cur_asset_model')\n",
    "print(cur_asset_model)\n",
    "print('cur_asset_model_v2')\n",
    "print(cur_asset_model_v2)\n",
    "print('cur_asset_human')\n",
    "print(cur_asset_human)\n",
    "print('cur_asset_sp500')\n",
    "print(cur_asset_sp500)\n",
    "print(\"Top 5 Model Contributors:\\n\", top5_model_contrib)\n",
    "print(\"Bottom 5 Model Contributors:\\n\", bottom5_model_contrib)\n",
    "print(\"Top 5 Human Contributors:\\n\", top5_human_contrib)\n",
    "print(\"Bottom 5 Human Contributors:\\n\", bottom5_human_contrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_asset_sp500_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_contrib(model_contrib_df):\n",
    "  # Calculate sum of contributions\n",
    "  total_model_contrib = model_contrib_df.sum()\n",
    "\n",
    "  # Get top 10 and bottom 10 contributors\n",
    "  top10_model_contrib = total_model_contrib[total_model_contrib>0].sort_values(ascending=False).head(10)\n",
    "  bottom10_model_contrib = total_model_contrib[total_model_contrib<0].sort_values().head(10)\n",
    "\n",
    "  # Combine top 10 and bottom 10 for plotting\n",
    "  combined_contrib = pd.concat([top10_model_contrib, bottom10_model_contrib])\n",
    "\n",
    "  # Create a bar chart\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  colors = ['red' if x < 0 else 'green' for x in combined_contrib.values]\n",
    "  combined_contrib.plot(kind='bar', color=colors)\n",
    "\n",
    "  # Adding titles and labels\n",
    "  plt.title('Top 10 and Bottom 10 Model Contributors')\n",
    "  plt.xlabel('Stocks')\n",
    "  plt.ylabel('Total Contributions')\n",
    "  plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero to separate negative and positive contributions\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_contrib(model_contrib_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_contrib(model_v2_contrib_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sum of contributions\n",
    "last_model_contrib = model_contrib_df.iloc[-1, :]\n",
    "sum_last_model_contrib = last_model_contrib.sum()\n",
    "\n",
    "# Get top 10 and bottom 10 contributors\n",
    "top10_model_contrib = last_model_contrib[last_model_contrib>0].sort_values(ascending=False).head(10)\n",
    "bottom10_model_contrib = last_model_contrib[last_model_contrib<0].sort_values().head(10)\n",
    "\n",
    "# Combine top 10 and bottom 10 for plotting\n",
    "combined_contrib = pd.concat([top10_model_contrib, bottom10_model_contrib])\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if x < 0 else 'green' for x in combined_contrib.values]\n",
    "combined_contrib.plot(kind='bar', color=colors)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(f'Top 10 and Bottom 10 Model Contributors for last day: {sum_last_model_contrib}')\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Last Contributions')\n",
    "plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero to separate negative and positive contributions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sum of contributions\n",
    "last_model_contrib = model_v2_contrib_df.iloc[-1, :]\n",
    "sum_last_model_contrib = last_model_contrib.sum()\n",
    "\n",
    "# Get top 10 and bottom 10 contributors\n",
    "top10_model_contrib = last_model_contrib[last_model_contrib>0].sort_values(ascending=False).head(10)\n",
    "bottom10_model_contrib = last_model_contrib[last_model_contrib<0].sort_values().head(10)\n",
    "\n",
    "# Combine top 10 and bottom 10 for plotting\n",
    "combined_contrib = pd.concat([top10_model_contrib, bottom10_model_contrib])\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if x < 0 else 'green' for x in combined_contrib.values]\n",
    "combined_contrib.plot(kind='bar', color=colors)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title(f'Top 10 and Bottom 10 Model Contributors for last day: {sum_last_model_contrib}')\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Last Contributions')\n",
    "plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero to separate negative and positive contributions\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sum of contributions\n",
    "total_human_contrib = human_contrib_df.sum()\n",
    "\n",
    "# Remove entries with very small absolute values\n",
    "significant_contrib = total_human_contrib[abs(total_human_contrib) > 0.001]\n",
    "\n",
    "# Get top 10 and bottom 10 significant contributors\n",
    "top10_human_contrib = significant_contrib[significant_contrib>0].sort_values(ascending=False).head(3)\n",
    "bottom10_human_contrib = significant_contrib[total_human_contrib<0].sort_values().head(3)\n",
    "\n",
    "# Combine the positive contributions on the left and negative on the right for plotting\n",
    "combined_contrib = pd.concat([top10_human_contrib, bottom10_human_contrib])\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(6, 3))\n",
    "# Sorting the colors based on the values being negative or positive\n",
    "colors = ['green' if x > 0 else 'red' for x in combined_contrib.values]\n",
    "combined_contrib.plot(kind='bar', color=colors)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Top 10 and Bottom 10 Human Contributors - Positives on Left (Excluding Small Values)')\n",
    "plt.xlabel('Stocks')\n",
    "plt.ylabel('Total Contributions')\n",
    "plt.axhline(0, color='gray', linewidth=0.8)  # Add a line at zero to separate negative and positive contributions\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vertical_line(fig, date, annotation, cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist):\n",
    "    # Adding a vertical line and annotation\n",
    "    combined_hist = cur_asset_model_hist + cur_asset_human_hist + cur_asset_sp500_hist\n",
    "    \n",
    "    fig.add_shape(\n",
    "        dict(\n",
    "            type=\"line\",\n",
    "            x0=date,\n",
    "            y0=min(combined_hist),\n",
    "            x1=date,\n",
    "            y1=max(combined_hist),\n",
    "            line=dict(\n",
    "                color=\"blue\",\n",
    "                width=1\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.add_annotation(\n",
    "        dict(\n",
    "            x=date,\n",
    "            y=max(combined_hist),\n",
    "            text=annotation,\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            ax=0,\n",
    "            ay=-40\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use plotly to plot the curve of the three assets\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "date_list = log_return.index.tolist()\n",
    "date_list.insert(0, pd.Timestamp('2024-05-14'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=date_list, y=cur_asset_model_hist, mode='lines', name='Model'))\n",
    "fig.add_trace(go.Scatter(x=date_list, y=cur_asset_model_v2_hist, mode='lines', name='Model v2'))\n",
    "fig.add_trace(go.Scatter(x=date_list, y=cur_asset_human_hist, mode='lines', name='Human'))\n",
    "fig.add_trace(go.Scatter(x=date_list, y=cur_asset_sp500_hist, mode='lines', name='SP500'))\n",
    "\n",
    "add_vertical_line(fig, '2024-07-20', 'global selectKBest', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "add_vertical_line(fig, '2024-08-08', '2xsigma', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "add_vertical_line(fig, '2024-08-19', 'A share', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "add_vertical_line(fig, '2024-09-06', 'Daily Update', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "add_vertical_line(fig, '2024-09-28', 'Stopped', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "add_vertical_line(fig, '2024-10-08', '1xsigma', cur_asset_model_hist, cur_asset_human_hist, cur_asset_sp500_hist)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = \"./processed_data_128\"\n",
    "S = pd.read_pickle(f'{data_dir}/S.pkl')\n",
    "mu = np.load(f'{data_dir}/mu.npy')\n",
    "\n",
    "# load valid_tickers.txt    \n",
    "with open(f'{data_dir}/valid_tickers.txt', 'r') as f:\n",
    "    valid_tickers = f.read().splitlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_tickers))\n",
    "print(len(mu))\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean and variance, the X axis is the std, the Y axis is the mean\n",
    "# the vaiance is the diagonal of the covariance matrix\n",
    "# if the ticker name contains .SS, it is a Chinese stock, mark it with red color\n",
    "# if the ticker name contains .DE, it is a German stock, mark\n",
    "# if the ticker name contains .L, it is a London stock, mark it with green color\n",
    "\n",
    "# Prepare the data for plotting\n",
    "standard_deviation = np.sqrt(np.diag(S))\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define a function to get marker styles based on ticker\n",
    "def get_marker_style(ticker):\n",
    "    if \".SS\" in ticker:\n",
    "        return dict(symbol='circle', color='red')\n",
    "    elif \".ST\" in ticker:\n",
    "        return dict(symbol='square', color='blue')\n",
    "    elif \".DE\" in ticker:\n",
    "        return dict(symbol='diamond', color='green')\n",
    "    elif \".L\" in ticker:\n",
    "        return dict(symbol='hexagon', color='purple')\n",
    "    else:\n",
    "        return dict(symbol='cross', color='black')\n",
    "\n",
    "# Add each ticker to the plot with appropriate marker style\n",
    "for i, ticker in enumerate(valid_tickers):\n",
    "    marker_style = get_marker_style(ticker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[standard_deviation[i]],\n",
    "        y=[mu[i]],\n",
    "        mode='markers',\n",
    "        marker=marker_style,\n",
    "        text=[ticker],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stocks Risk and Return',\n",
    "    yaxis_title='Expected Return',\n",
    "    xaxis_title='Standard Deviation'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "# Defining the objective function and constraints\n",
    "def portfolio_variance(weights, mu_tensor, S_tensor):\n",
    "    return torch.dot(weights.T, torch.matmul(S_tensor, weights))\n",
    "\n",
    "def optimize_portfolio(mean, mu_tensor, S_tensor, num_assets, bounds_tensor):\n",
    "    # Initial guess\n",
    "    weights = torch.full((num_assets,), 1 / num_assets, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([weights], lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n",
    "\n",
    "    for _ in range(1500):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        var = portfolio_variance(weights, mu_tensor, S_tensor)\n",
    "        mean_return = torch.dot(weights, mu_tensor)\n",
    "        constraint_loss = (mean_return - mean) ** 2\n",
    "        unit_sum_constraint = (torch.sum(weights) - 1) ** 2\n",
    "        loss = var + constraint_loss + unit_sum_constraint\n",
    "        loss.backward()\n",
    "\n",
    "        # Applying bounds\n",
    "        with torch.no_grad():\n",
    "            weights.clamp_(bounds_tensor[:, 0], bounds_tensor[:, 1])\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "    return weights.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_portfolio_2(mean, mu_tensor, S_tensor, num_assets, bounds_tensor):\n",
    "    # Initial guess\n",
    "    weights = torch.full((num_assets,), 1 / num_assets, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam([weights], lr=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n",
    "    lower_bound_tensor = torch.zeros(num_assets)  # `0.0` tensor for the lower bound\n",
    "\n",
    "    for _ in range(1500):  # Number of iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        var = portfolio_variance(weights, mu_tensor, S_tensor)\n",
    "        mean_return = torch.dot(weights, mu_tensor)\n",
    "        constraint_loss = (mean_return - mean) ** 2\n",
    "        unit_sum_constraint = (torch.sum(weights) - 1) ** 2\n",
    "        loss = var + constraint_loss + unit_sum_constraint\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Manually applying complex bounds\n",
    "        with torch.no_grad():\n",
    "            weights.clamp_(lower_bound_tensor, bounds_tensor[:, 1])  # Initially, apply simple upper bound\n",
    "            # Applying zero or lower limit of the range (0.02)\n",
    "            weights.copy_(torch.where((weights > 0) & (weights < 0.02), torch.full_like(weights, 0.02), weights))\n",
    "            weights.copy_(torch.where(weights < 0.01, torch.zeros_like(weights), weights))\n",
    "\n",
    "        scheduler.step(loss)\n",
    "\n",
    "    return weights.detach().numpy()\n",
    "\n",
    "\n",
    "num_assets = len(valid_tickers)\n",
    "# Example bounds setup where weight is 0 or between 0.02 and 0.2\n",
    "bounds = np.array([(0, 0.2)] * num_assets)  # Although redundant if manually handling bounds in the loop\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'S' is a pandas DataFrame\n",
    "import torch\n",
    "\n",
    "S_tensor = torch.tensor(S.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor = torch.tensor(mu, dtype=torch.float32)\n",
    "num_assets = len(mu)\n",
    "bounds = np.array([(0.0, 0.2)] * num_assets, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "\n",
    "\n",
    "efficient_means = np.linspace(-0.1, 0.3, 16)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios = []\n",
    "\n",
    "for mean in efficient_means:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor, S_tensor, num_assets, bounds_tensor)\n",
    "    efficient_portfolios.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor, S_tensor).sqrt().item() for portfolio in efficient_portfolios]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks, efficient_means)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.02\n",
    "period = 128\n",
    "\n",
    "\n",
    "sharpe_ratios = [(y - risk_free_rate) / x for x, y in zip(efficient_risks, efficient_means)]\n",
    "max_sharpe_idx = np.argmax(sharpe_ratios)\n",
    "\n",
    "weights = efficient_portfolios[max_sharpe_idx]\n",
    "\n",
    "tickers_to_buy = []\n",
    "tot_weight = 0\n",
    "for index, ticker_name in enumerate(valid_tickers):\n",
    "    weight = float(weights[index])\n",
    "\n",
    "    if weight > 1e-3:\n",
    "        print(f'index: {index} {ticker_name}: weight {weight} exp profit: {mu[index]}, variance: {S[ticker_name][ticker_name]}')\n",
    "        ticker_info = {'id': ticker_name, 'weight': weight}\n",
    "        tickers_to_buy.append(ticker_info)\n",
    "        tot_weight += weight\n",
    "\n",
    "print(tot_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import minimize\n",
    "import concurrent.futures\n",
    "\n",
    "# Prepare the data for plotting\n",
    "standard_deviation = np.sqrt(np.diag(S))\n",
    "expected_returns = mu\n",
    "\n",
    "# Define a function to get marker styles based on ticker\n",
    "def get_marker_style(ticker):\n",
    "    if \".SS\" in ticker:\n",
    "        return dict(symbol='circle', color='red')\n",
    "    elif \".ST\" in ticker:\n",
    "        return dict(symbol='square', color='blue')\n",
    "    elif \".DE\" in ticker:\n",
    "        return dict(symbol='diamond', color='green')\n",
    "    elif \".L\" in ticker:\n",
    "        return dict(symbol='hexagon', color='purple')\n",
    "    else:\n",
    "        return dict(symbol='cross', color='black')\n",
    "\n",
    "# Add individual stocks\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, ticker in enumerate(valid_tickers):\n",
    "    marker_style = get_marker_style(ticker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[standard_deviation[i]],\n",
    "        y=[expected_returns[i]],\n",
    "        mode='markers',\n",
    "        marker=marker_style,\n",
    "        text=[ticker],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Define portfolio statistics functions\n",
    "def portfolio_stats(weights, mu, cov_matrix):\n",
    "    expected_return = np.dot(weights, mu)\n",
    "    expected_variance = np.dot(weights.T, np.dot(cov_matrix, weights))\n",
    "    expected_std_dev = np.sqrt(expected_variance)\n",
    "    return expected_return, expected_variance, expected_std_dev\n",
    "\n",
    "# Number of portfolios to simulate\n",
    "num_portfolios = 10000\n",
    "num_assets = len(mu)\n",
    "results = np.zeros((3, num_portfolios))\n",
    "\n",
    "# Define the risk-free rate (for example, assume 0)\n",
    "risk_free_rate = 0.02\n",
    "\n",
    "# Optimize portfolios to draw efficient frontier\n",
    "def min_variance(weights, mu, S):\n",
    "    return portfolio_stats(weights, mu, S)[2] ** 2\n",
    "\n",
    "\n",
    "\n",
    "def optimize_portfolio(mean, mu, S, num_assets, bounds):\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1},\n",
    "                   {'type': 'eq', 'fun': lambda x: portfolio_stats(x, mu, S)[0] - mean})\n",
    "    result = minimize(min_variance, num_assets * [1./num_assets,], args=(mu, S),\n",
    "                       method='SLSQP', bounds=bounds, constraints=constraints, options={\"maxiter\": 200})\n",
    "    return result\n",
    "\n",
    "constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "bounds = tuple((0.0,0.2) for _ in range(num_assets))\n",
    "efficient_means = np.linspace(-0.1, 0.3, 32)\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the computation\n",
    "efficient_portfolios = []\n",
    "total_tasks = len(efficient_means)\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize the computation\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    future_to_mean = {executor.submit(optimize_portfolio, mean, mu, S, num_assets, bounds): mean for mean in efficient_means}\n",
    "    completed_tasks = 0\n",
    "    for future in concurrent.futures.as_completed(future_to_mean):\n",
    "        completed_tasks += 1\n",
    "        print(f'{completed_tasks}/{total_tasks} tasks completed')\n",
    "        efficient_portfolios.append(future.result())\n",
    "\n",
    "\n",
    "efficient_risks = [portfolio['fun'] ** 0.5 for portfolio in efficient_portfolios]\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks,\n",
    "    y=efficient_means,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=3),\n",
    "    name='Efficient Frontier'\n",
    "))\n",
    "\n",
    "\n",
    "# Calculate Sharpe Ratios for efficient portfolios\n",
    "risk_free_rate = 0.02\n",
    "sharpe_ratios = [(y - risk_free_rate) / x for x, y in zip(efficient_risks, efficient_means)]\n",
    "max_sharpe_idx = np.argmax(sharpe_ratios)\n",
    "\n",
    "# Tangency point on the efficient frontier\n",
    "tangency_risk = efficient_risks[max_sharpe_idx]\n",
    "tangency_return = efficient_means[max_sharpe_idx]\n",
    "tangency_portfolio = efficient_portfolios[max_sharpe_idx]\n",
    "\n",
    "\n",
    "# Plot the tangency point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[tangency_risk],\n",
    "    y=[tangency_return],\n",
    "    mode='markers',\n",
    "    marker=dict(color='gold', size=10, symbol='star'),\n",
    "    name='Tangency Portfolio'\n",
    "))\n",
    "\n",
    "# Plot the Capital Market Line (CML)\n",
    "cml_x = [0, tangency_risk * 1.5]  # Extend the CML to the right of the tangency point\n",
    "cml_y = [risk_free_rate, risk_free_rate + (tangency_return - risk_free_rate) / tangency_risk * (tangency_risk * 1.5)]\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cml_x,\n",
    "    y=cml_y,\n",
    "    mode='lines',\n",
    "    line=dict(color='orange', width=2, dash='dash'),\n",
    "    name='Capital Market Line (CML)'\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stocks Risk and Return and Efficient Frontier',\n",
    "    yaxis_title='Expected Return',\n",
    "    xaxis_title='Standard Deviation'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_volatility(weights, covariance_log_returns):\n",
    "    covariance_returns = np.exp(covariance_log_returns) - 1\n",
    "    return np.sqrt(np.dot(weights.T, np.dot(covariance_returns, weights)))\n",
    "\n",
    "def portfolio_return(weights, log_returns, allow_short=False):\n",
    "    returns = np.exp(log_returns) - 1\n",
    "    return np.sum(np.abs(returns)*weights) if allow_short else np.sum(returns*weights)\n",
    "\n",
    "period = 128\n",
    "\n",
    "weights = tangency_portfolio.x\n",
    "tickers_to_buy = []\n",
    "tot_weight = 0\n",
    "\n",
    "for index, ticker_name in enumerate(valid_tickers):\n",
    "  weight = float(weights[index])\n",
    "  tot_weight += weight\n",
    "  if weight > 1e-2:\n",
    "    print(f'index: {index} {ticker_name}: weight {weight} exp profit: {mu[index]}, variance: {S[ticker_name][ticker_name]}')\n",
    "    ticker_info = {'id': ticker_name, 'weight': weight}\n",
    "    tickers_to_buy.append(ticker_info)\n",
    "  \n",
    "\n",
    "print(f'expected return in {period} trading days: {portfolio_return(weights, mu)}')\n",
    "print(f'volatility of the return in {period} trading days: {portfolio_volatility(weights, S)}')\n",
    "print(tot_weight)\n",
    "# print tickers_to_buy in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tangency_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the names in the valid_tickers, filter out the stocks with .SS as suffix\n",
    "import torch\n",
    "\n",
    "\n",
    "S_china = S.copy()\n",
    "mu_china = mu.copy()\n",
    "\n",
    "# get the index of the stocks with .SS as suffix\n",
    "china_idx = [i for i, ticker in enumerate(valid_tickers) if \".SS\" in ticker]\n",
    "\n",
    "# filter out the stocks with .SS as suffix\n",
    "S_china = S_china.iloc[china_idx, china_idx]\n",
    "mu_china = mu_china[china_idx]\n",
    "\n",
    "# Assuming 'S' is a pandas DataFrame\n",
    "S_tensor_china = torch.tensor(S_china.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor_china = torch.tensor(mu_china, dtype=torch.float32)\n",
    "num_assets_china = len(mu_china)\n",
    "# Configure optimization constraints and bounds\n",
    "bounds = np.array([(0.02, 0.2)] * num_assets_china, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "efficient_means_china = np.linspace(-0.1, 0.3, 32)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios_china = []\n",
    "\n",
    "for mean in efficient_means_china:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor_china, S_tensor_china, num_assets_china, bounds_tensor)\n",
    "    efficient_portfolios_china.append(result)\n",
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks_china = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor_china, S_tensor_china).sqrt().item() for portfolio in efficient_portfolios_china]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks_china, efficient_means_china)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the names in the valid_tickers, filter out the stocks with .SS as suffix\n",
    "import torch\n",
    "\n",
    "\n",
    "S_sweden = S.copy()\n",
    "mu_sweden = mu.copy()\n",
    "\n",
    "# get the index of the stocks with .SS as suffix\n",
    "sweden_idx = [i for i, ticker in enumerate(valid_tickers) if \".ST\" in ticker]\n",
    "\n",
    "# filter out the stocks with .SS as suffix\n",
    "S_sweden = S_sweden.iloc[sweden_idx, sweden_idx]\n",
    "mu_sweden = mu_sweden[sweden_idx]\n",
    "\n",
    "# Assuming 'S' is a pandas DataFrame\n",
    "S_tensor_sweden = torch.tensor(S_sweden.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor_sweden = torch.tensor(mu_sweden, dtype=torch.float32)\n",
    "num_assets_sweden = len(mu_sweden)\n",
    "# Configure optimization constraints and bounds\n",
    "bounds = np.array([(0, 0.2)] * num_assets_sweden, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "efficient_means_sweden = np.linspace(-0.1, 0.3, 32)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios_sweden = []\n",
    "\n",
    "for mean in efficient_means_sweden:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor_sweden, S_tensor_sweden, num_assets_sweden, bounds_tensor)\n",
    "    efficient_portfolios_sweden.append(result)\n",
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks_sweden = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor_sweden, S_tensor_sweden).sqrt().item() for portfolio in efficient_portfolios_sweden]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks_sweden, efficient_means_sweden)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the names in the valid_tickers, filter out the stocks with .SS as suffix\n",
    "import torch\n",
    "\n",
    "\n",
    "S_germany = S.copy()\n",
    "mu_germany = mu.copy()\n",
    "\n",
    "# get the index of the stocks with .SS as suffix\n",
    "germany_idx = [i for i, ticker in enumerate(valid_tickers) if \".DE\" in ticker]\n",
    "\n",
    "# filter out the stocks with .SS as suffix\n",
    "S_germany = S_germany.iloc[germany_idx, germany_idx]\n",
    "mu_germany = mu_germany[germany_idx]\n",
    "\n",
    "# Assuming 'S' is a pandas DataFrame\n",
    "S_tensor_germany = torch.tensor(S_germany.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor_germany = torch.tensor(mu_germany, dtype=torch.float32)\n",
    "num_assets_germany = len(mu_germany)\n",
    "# Configure optimization constraints and bounds\n",
    "bounds = np.array([(0, 0.2)] * num_assets_germany, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "efficient_means_germany = np.linspace(-0.1, 0.3, 32)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios_germany = []\n",
    "\n",
    "for mean in efficient_means_germany:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor_germany, S_tensor_germany, num_assets_germany, bounds_tensor)\n",
    "    efficient_portfolios_germany.append(result)\n",
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks_germany = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor_germany, S_tensor_germany).sqrt().item() for portfolio in efficient_portfolios_germany]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks_germany, efficient_means_germany)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the names in the valid_tickers, filter out the stocks with .SS as suffix\n",
    "import torch\n",
    "\n",
    "\n",
    "S_uk = S.copy()\n",
    "mu_uk = mu.copy()\n",
    "\n",
    "# get the index of the stocks with .SS as suffix\n",
    "uk_idx = [i for i, ticker in enumerate(valid_tickers) if \".L\" in ticker]\n",
    "\n",
    "# filter out the stocks with .SS as suffix\n",
    "S_uk = S_uk.iloc[uk_idx, uk_idx]\n",
    "mu_uk = mu_uk[uk_idx]\n",
    "\n",
    "# Assuming 'S' is a pandas DataFrame\n",
    "S_tensor_uk = torch.tensor(S_uk.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor_uk = torch.tensor(mu_uk, dtype=torch.float32)\n",
    "num_assets_uk = len(mu_uk)\n",
    "# Configure optimization constraints and bounds\n",
    "bounds = np.array([(0, 0.2)] * num_assets_uk, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "efficient_means_uk = np.linspace(-0.1, 0.3, 32)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios_uk = []\n",
    "\n",
    "for mean in efficient_means_uk:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor_uk, S_tensor_uk, num_assets_uk, bounds_tensor)\n",
    "    efficient_portfolios_uk.append(result)\n",
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks_uk = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor_uk, S_tensor_uk).sqrt().item() for portfolio in efficient_portfolios_uk]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks_uk, efficient_means_uk)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the names in the valid_tickers, filter out the stocks with .SS as suffix\n",
    "import torch\n",
    "\n",
    "\n",
    "S_us = S.copy()\n",
    "mu_us = mu.copy()\n",
    "\n",
    "# get the index of the stocks without suffix\n",
    "us_idx = [i for i, ticker in enumerate(valid_tickers) if \".\" not in ticker]\n",
    "\n",
    "\n",
    "# filter out the stocks with .SS as suffix\n",
    "S_us = S_us.iloc[us_idx, us_idx]\n",
    "mu_us = mu_us[us_idx]\n",
    "\n",
    "# Assuming 'S' is a pandas DataFrame\n",
    "S_tensor_us = torch.tensor(S_us.values, dtype=torch.float32)\n",
    "# Assuming 'mu' is a NumPy array\n",
    "mu_tensor_us = torch.tensor(mu_us, dtype=torch.float32)\n",
    "num_assets_us = len(mu_us)\n",
    "# Configure optimization constraints and bounds\n",
    "bounds = np.array([(0, 0.2)] * num_assets_us, dtype=np.float32)\n",
    "bounds_tensor = torch.tensor(bounds, dtype=torch.float32)\n",
    "efficient_means_us = np.linspace(-0.1, 0.3, 32)  # Custom range of target returns for the efficient frontier\n",
    "\n",
    "# Run optimization sequentially\n",
    "efficient_portfolios_us = []\n",
    "\n",
    "for mean in efficient_means_us:\n",
    "    print(f'Optimizing for target mean return: {mean}')\n",
    "    result = optimize_portfolio(mean, mu_tensor_us, S_tensor_us, num_assets_us, bounds_tensor)\n",
    "    efficient_portfolios_us.append(result)\n",
    "\n",
    "# Convert portfolios to Torch tensors and calculate portfolio variances\n",
    "efficient_risks_us = [portfolio_variance(torch.tensor(portfolio, dtype=torch.float32), mu_tensor_us, S_tensor_us).sqrt().item() for portfolio in efficient_portfolios_us]\n",
    "\n",
    "# Print out results\n",
    "for i, (risk, mean_return) in enumerate(zip(efficient_risks_us, efficient_means_us)):\n",
    "    print(f'Portfolio {i+1}: Target Mean Return = {mean_return:.4f}, Standard Deviation = {risk:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add individual stocks\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks,\n",
    "    y=efficient_means,\n",
    "    mode='lines',\n",
    "    line=dict(color='black', width=3),\n",
    "    name='Efficient Frontier'\n",
    "))\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks_china,\n",
    "    y=efficient_means_china,\n",
    "    mode='lines',\n",
    "    line=dict(color='red', width=3),\n",
    "    name='Efficient Frontier China'\n",
    "))\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks_us,\n",
    "    y=efficient_means_us,\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=3),\n",
    "    name='Efficient Frontier US'\n",
    "))\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks_uk,\n",
    "    y=efficient_means_uk,\n",
    "    mode='lines',\n",
    "    line=dict(color='green', width=3),\n",
    "    name='Efficient Frontier UK'\n",
    "))\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks_sweden,\n",
    "    y=efficient_means_sweden,\n",
    "    mode='lines',\n",
    "    line=dict(color='yellow', width=3),\n",
    "    name='Efficient Frontier Sweden'\n",
    "))\n",
    "\n",
    "# Plot the efficient frontier\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=efficient_risks_germany,\n",
    "    y=efficient_means_germany,\n",
    "    mode='lines',\n",
    "    line=dict(color='orange', width=3),\n",
    "    name='Efficient Frontier Germany'\n",
    "))\n",
    "\n",
    "# Calculate Sharpe Ratios for efficient portfolios\n",
    "risk_free_rate = 0.02\n",
    "sharpe_ratios = [(y - risk_free_rate) / x for x, y in zip(efficient_risks, efficient_means)]\n",
    "max_sharpe_idx = np.argmax(sharpe_ratios)\n",
    "\n",
    "# Tangency point on the efficient frontier\n",
    "tangency_risk = efficient_risks[max_sharpe_idx]\n",
    "tangency_return = efficient_means[max_sharpe_idx]\n",
    "\n",
    "# Plot the tangency point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[tangency_risk],\n",
    "    y=[tangency_return],\n",
    "    mode='markers',\n",
    "    marker=dict(color='gold', size=10, symbol='star'),\n",
    "    name='Tangency Portfolio'\n",
    "))\n",
    "\n",
    "# Plot the Capital Market Line (CML)\n",
    "cml_x = [0, tangency_risk * 1.5]  # Extend the CML to the right of the tangency point\n",
    "cml_y = [risk_free_rate, risk_free_rate + (tangency_return - risk_free_rate) / tangency_risk * (tangency_risk * 1.5)]\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cml_x,\n",
    "    y=cml_y,\n",
    "    mode='lines',\n",
    "    line=dict(color='blue', width=2, dash='dash'),\n",
    "    name='Capital Market Line'\n",
    "))\n",
    "\n",
    "# Plot the Capital Market Line (CML) for China\n",
    "# Tangency point on the efficient frontier\n",
    "sharpe_ratios_china = [(y - risk_free_rate) / x for x, y in zip(efficient_risks_china, efficient_means_china)]\n",
    "max_sharpe_idx_china = np.argmax(sharpe_ratios_china)\n",
    "\n",
    "tangency_risk_china = efficient_risks_china[max_sharpe_idx_china]\n",
    "tangency_return_china = efficient_means_china[max_sharpe_idx_china]\n",
    "\n",
    "# Plot the tangency point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[tangency_risk_china],\n",
    "    y=[tangency_return_china],\n",
    "    mode='markers',\n",
    "    marker=dict(color='gold', size=10, symbol='star'),\n",
    "    name='Tangency Portfolio'\n",
    "))\n",
    "\n",
    "# Plot the Capital Market Line (CML)\n",
    "cml_x_china = [0, tangency_risk_china * 1.5]  # Extend the CML to the right of the tangency point\n",
    "cml_y_china = [risk_free_rate, risk_free_rate + (tangency_return_china - risk_free_rate) / tangency_risk_china * (tangency_risk_china * 1.5)]\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=cml_x_china,\n",
    "    y=cml_y_china,\n",
    "    mode='lines',\n",
    "    line=dict(color='orange', width=2, dash='dash'),\n",
    "    name='Capital Market Line China(CML)'\n",
    "))\n",
    "\n",
    "# Prepare the data for plotting\n",
    "standard_deviation = np.sqrt(np.diag(S))\n",
    "expected_returns = mu\n",
    "\n",
    "for i, ticker in enumerate(valid_tickers):\n",
    "    marker_style = get_marker_style(ticker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[standard_deviation[i]],\n",
    "        y=[expected_returns[i]],\n",
    "        mode='markers',\n",
    "        marker=marker_style,\n",
    "        text=[ticker],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stocks Risk and Return and Efficient Frontier',\n",
    "    yaxis_title='Expected Return',\n",
    "    xaxis_title='Standard Deviation'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter valid_tickers to only include those in log_return's columns\n",
    "filtered_tickers = [ticker for ticker in valid_tickers if ticker in log_return.columns]\n",
    "\n",
    "# Get the indices of the filtered tickers in S.columns\n",
    "ticker_indices = [S.columns.get_loc(ticker) for ticker in filtered_tickers]\n",
    "\n",
    "# Prepare the data for plotting\n",
    "standard_deviation = np.sqrt(np.diag(S))[ticker_indices]\n",
    "expected_returns = mu[ticker_indices]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Define a function to get marker styles based on ticker\n",
    "def get_marker_style(ticker):\n",
    "    if \".SS\" in ticker:\n",
    "        return dict(symbol='circle', color='red')\n",
    "    elif \".ST\" in ticker:\n",
    "        return dict(symbol='square', color='blue')\n",
    "    elif \".DE\" in ticker:\n",
    "        return dict(symbol='diamond', color='green')\n",
    "    elif \".L\" in ticker:\n",
    "        return dict(symbol='hexagon', color='purple')\n",
    "    else:\n",
    "        return dict(symbol='cross', color='black')\n",
    "\n",
    "# Add each ticker to the plot with appropriate marker style\n",
    "for i, ticker in enumerate(filtered_tickers):\n",
    "    marker_style = get_marker_style(ticker)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[standard_deviation[i]],\n",
    "        y=[expected_returns[i]],\n",
    "        mode='markers',\n",
    "        marker=marker_style,\n",
    "        text=[ticker],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Stocks Risk and Return',\n",
    "    yaxis_title='Expected Return',\n",
    "    xaxis_title='Standard Deviation'\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = log_return.index.tolist()\n",
    "date_list.insert(0, pd.Timestamp('2024-05-14'))\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur_asset = 1\n",
    "weights = np.zeros(len(unique_ids))\n",
    "for date in log_return.index:\n",
    "  # get the date as a string\n",
    "  date_str = str(date.date())\n",
    "  print(date_str)\n",
    "  # enumerate the trade log to see if the date is in the model_trace_log\n",
    "  for log in human_brain_trace_log:\n",
    "    if log['date'] == date_str:\n",
    "      print(log['weights'])\n",
    "      # build the vector of weights\n",
    "      # we need to update weights\n",
    "      for asset in log['weights']:\n",
    "        asset_id = asset['id']\n",
    "        asset_weight = asset['weight']\n",
    "        asset_index = list(unique_ids).index(asset_id)\n",
    "        weights[asset_index] = asset_weight\n",
    "  # get the daily profit\n",
    "  daily_log_return = np.dot(weights, log_return.loc[date].values)\n",
    "  cur_asset = cur_asset * np.exp(daily_log_return)\n",
    "  # if the date is in the human_brain_trace\n",
    "\n",
    "print(cur_asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yfin\n",
    "import datetime\n",
    "\n",
    "yfin.pdr_override()\n",
    "\n",
    "# Extract unique stock ids and the circular dates needed for periods\n",
    "unique_ids = set()\n",
    "dates = [log['date'] for log in model_trace_log]\n",
    "\n",
    "# Get the start and end date range for the data retrieval\n",
    "start_date = pd.to_datetime(min(dates))\n",
    "end_date = pd.to_datetime(max(dates)) + pd.DateOffset(days=1)  # ensuring coverage for the last date's day\n",
    "\n",
    "# List to store current date + next date for holding period calculations\n",
    "date_ranges = [(pd.to_datetime(model_trace_log[i]['date']), pd.to_datetime(model_trace_log[i+1]['date']) if i+1 < len(model_trace_log) else pd.to_datetime('now'))\n",
    "               for i in range(len(model_trace_log))]\n",
    "\n",
    "# Get unique stock ids from all logs\n",
    "for log in model_trace_log:\n",
    "    for weight in log['weights']:\n",
    "        unique_ids.add(weight['id'])\n",
    "\n",
    "# Load stock data\n",
    "stocks_data = pdr.get_data_yahoo(list(unique_ids), start=start_date, end=datetime.datetime.now())\n",
    "\n",
    "# Fetch the S&P 500 data for the whole period\n",
    "sp500_data = pdr.get_data_yahoo('^GSPC', start=start_date, end=datetime.datetime.now())\n",
    "\n",
    "# Calculate the weighted stock prices over time for each log segment\n",
    "portfolio_value = pd.Series(index=stocks_data['Adj Close'].index)\n",
    "\n",
    "for start_date, end_date in date_ranges:\n",
    "    for log in model_trace_log:\n",
    "        if pd.to_datetime(log['date']) == start_date:\n",
    "            for asset in log['weights']:\n",
    "                weight = asset['weight']\n",
    "                stock_id = asset['id']\n",
    "                if start_date in stocks_data['Adj Close'].index and stock_id in stocks_data['Adj Close'].columns:\n",
    "                    price_data = stocks_data['Adj Close'][stock_id].loc[start_date:end_date].fillna(method='ffill')\n",
    "                    weighted_prices = price_data * weight\n",
    "                    if portfolio_value[start_date:end_date].empty:\n",
    "                        portfolio_value[start_date:end_date] = weighted_prices\n",
    "                    else:\n",
    "                        portfolio_value[start_date:end_date] += weighted_prices\n",
    "\n",
    "# Normalize both the portfolio's total value and the S&P 500 to start at 100 (relative comparison)\n",
    "portfolio_value.dropna(inplace=True)  # Remove any NaN values\n",
    "portfolio_growth = (portfolio_value / portfolio_value.iloc[0]) * 100\n",
    "sp500_growth = (sp500_data['Adj Close'] / sp500_data['Adj Close'][0]) * 100\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(portfolio_growth.index, portfolio_growth, label='Model Portfolio')\n",
    "plt.plot(sp500_growth.index, sp500_growth, label='S&P 500', color='red', linestyle='--')\n",
    "plt.title('Accumulated Portfolio Growth vs. S&P 500')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
