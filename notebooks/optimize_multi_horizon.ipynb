{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error for time horizon 64: 0.006851094008759641\n",
      "Average error for time horizon 128: 0.03363520184794312\n",
      "Average error for time horizon 256: 0.02844700424454357\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "time_horizons = [64,128, 256]\n",
    "\n",
    "all_errors = None\n",
    "all_mu = None\n",
    "for time_horizon in time_horizons:\n",
    "  data_dir = f'../processed_data_{time_horizon}'\n",
    "  df = pd.read_csv(os.path.join(data_dir, 'all_errors.csv'),  index_col=0, parse_dates=True)\n",
    "  # compute the mean of the errors\n",
    "  avg_err = df.mean(axis=1).mean()\n",
    "  df = df * np.sqrt(256 / time_horizon)\n",
    "  print(f'Average error for time horizon {time_horizon}: {avg_err}')\n",
    "\n",
    "  df = df.add_suffix(f'_{time_horizon}')\n",
    "  mu = np.load(os.path.join(data_dir, 'mu.npy'))\n",
    "  mu = mu * (256 / time_horizon)\n",
    "\n",
    "  if all_errors is None:\n",
    "    all_errors = df\n",
    "  else:\n",
    "    all_errors = pd.concat([all_errors, df], axis=1, join='outer')\n",
    "\n",
    "  if all_mu is None:\n",
    "    all_mu = mu\n",
    "  else:\n",
    "    all_mu = np.concatenate([all_mu, mu])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1461, 2222)\n",
      "(2222,)\n"
     ]
    }
   ],
   "source": [
    "print(all_errors.shape)\n",
    "print(all_mu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of assets with mu < mean(mu): 750\n",
      "[   0    1    3    4    6    9   11   12   15   18   19   24   25   27\n",
      "   29   31   32   33   35   36   48   50   54   55   56   59   65   68\n",
      "   71   75   76   77   79   80   82   85   89   91   92   93   94   96\n",
      "  100  103  104  105  109  110  111  114  115  119  120  122  123  124\n",
      "  133  135  136  137  138  148  152  153  161  164  171  178  182  183\n",
      "  185  187  190  192  193  206  207  210  212  214  218  220  228  231\n",
      "  234  236  237  241  246  249  252  258  260  264  265  266  267  273\n",
      "  278  280  286  289  292  295  297  301  305  307  312  313  315  317\n",
      "  327  332  334  335  342  343  346  351  353  354  355  356  358  361\n",
      "  383  385  398  404  415  418  419  426  442  458  463  469  476  487\n",
      "  493  494  502  504  510  513  514  523  524  531  542  543  550  551\n",
      "  559  566  568  569  571  581  585  586  595  596  597  598  600  602\n",
      "  606  608  609  611  612  615  616  617  622  625  626  629  631  637\n",
      "  639  644  645  646  647  650  668  673  678  680  696  707  712  713\n",
      "  724  728  733  740  741  742  744  745  747  750  752  753  759  760\n",
      "  761  763  765  766  768  770  772  773  774  776  777  780  787  788\n",
      "  789  791  793  795  796  797  799  800  807  808  809  810  812  813\n",
      "  816  817  818  819  820  821  822  823  826  830  832  833  834  835\n",
      "  837  838  839  841  844  845  846  850  851  852  855  856  858  860\n",
      "  861  862  863  864  865  866  872  874  876  877  878  879  889  902\n",
      "  905  912  915  919  920  922  923  925  926  927  931  933  934  936\n",
      "  943  947  948  955  958  959  961  969  971  972  975  977  978  979\n",
      "  982  983  987  989  990  991  993  995  999 1001 1005 1006 1007 1011\n",
      " 1013 1014 1016 1019 1021 1027 1030 1031 1032 1033 1035 1036 1038 1040\n",
      " 1042 1046 1048 1049 1053 1054 1056 1058 1062 1068 1069 1073 1075 1076\n",
      " 1083 1084 1087 1091 1092 1094 1095 1096 1097 1099 1100 1101 1102 1104\n",
      " 1107 1114 1118 1123 1124 1126 1130 1131 1133 1137 1139 1145 1146 1154\n",
      " 1156 1158 1159 1160 1167 1172 1173 1177 1180 1183 1190 1199 1204 1209\n",
      " 1210 1211 1213 1216 1217 1228 1231 1234 1235 1236 1239 1240 1245 1249\n",
      " 1250 1251 1254 1255 1256 1263 1265 1266 1267 1269 1272 1283 1284 1286\n",
      " 1287 1288 1290 1291 1292 1297 1302 1307 1309 1310 1312 1313 1322 1326\n",
      " 1327 1331 1335 1336 1337 1338 1339 1340 1341 1343 1344 1345 1347 1348\n",
      " 1349 1350 1352 1353 1356 1357 1358 1362 1363 1364 1366 1367 1370 1372\n",
      " 1373 1376 1378 1380 1385 1387 1388 1391 1392 1406 1407 1408 1414 1418\n",
      " 1420 1421 1423 1436 1447 1452 1453 1468 1470 1472 1473 1481 1482 1484\n",
      " 1485 1490 1492 1493 1496 1499 1500 1502 1503 1505 1506 1508 1510 1512\n",
      " 1513 1514 1516 1517 1525 1528 1531 1535 1536 1537 1539 1540 1548 1549\n",
      " 1555 1556 1557 1558 1560 1562 1563 1566 1570 1572 1573 1574 1575 1577\n",
      " 1578 1581 1584 1585 1586 1590 1592 1595 1600 1601 1603 1604 1605 1612\n",
      " 1614 1616 1618 1619 1628 1629 1633 1634 1645 1649 1652 1655 1659 1660\n",
      " 1663 1665 1666 1667 1668 1671 1673 1674 1676 1683 1685 1687 1688 1691\n",
      " 1695 1698 1699 1701 1709 1712 1715 1717 1718 1719 1722 1727 1730 1733\n",
      " 1739 1745 1746 1747 1748 1754 1756 1759 1760 1761 1767 1771 1772 1773\n",
      " 1775 1776 1778 1780 1782 1786 1788 1793 1802 1805 1808 1812 1813 1815\n",
      " 1816 1823 1827 1832 1834 1835 1836 1837 1839 1842 1844 1856 1857 1863\n",
      " 1864 1866 1870 1877 1879 1885 1886 1889 1892 1898 1899 1900 1907 1910\n",
      " 1912 1913 1917 1921 1923 1939 1944 1949 1950 1951 1952 1953 1956 1957\n",
      " 1968 1969 1971 1973 1974 1982 1983 1985 1990 1994 1995 1996 2003 2006\n",
      " 2011 2012 2023 2024 2026 2027 2028 2031 2032 2037 2038 2040 2042 2047\n",
      " 2049 2050 2053 2062 2063 2066 2067 2071 2075 2076 2077 2078 2079 2081\n",
      " 2083 2084 2085 2086 2087 2088 2089 2090 2092 2093 2096 2097 2098 2103\n",
      " 2106 2107 2110 2112 2118 2120 2125 2127 2128 2131 2142 2143 2147 2148\n",
      " 2149 2154 2155 2159 2160 2161 2162 2164 2171 2177 2180 2185 2188 2192\n",
      " 2193 2194 2196 2209 2210 2211 2213 2221]\n"
     ]
    }
   ],
   "source": [
    "# find out the indice of assets in mu that is less than mean(mu)\n",
    "mean_mu = all_mu.mean()\n",
    "idx = np.where(all_mu < 0)[0]\n",
    "print(f'Number of assets with mu < mean(mu): {len(idx)}')\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove the assets with mu < mean(mu) in both of mu and all_errors\n",
    "# all_mu = np.delete(all_mu, idx)\n",
    "# all_errors = all_errors.drop(all_errors.columns[idx], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2222\n",
      "(1461, 2222)\n"
     ]
    }
   ],
   "source": [
    "print(len(all_mu))\n",
    "print(all_errors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75225/3200124498.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  all_errors = all_errors.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "def get_shrinkage_covariance(df):\n",
    "    lw = LedoitWolf(store_precision=False, assume_centered=True)\n",
    "    lw.fit(df)\n",
    "    # Convert the ndarray back to a DataFrame and use the column and index from the original DataFrame\n",
    "    shrink_cov = pd.DataFrame(lw.covariance_, index=df.columns, columns=df.columns)\n",
    "    return shrink_cov\n",
    "\n",
    "all_errors = all_errors.fillna(method='ffill').fillna(method='bfill')\n",
    "S = get_shrinkage_covariance(all_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 2222)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2222, 2222)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def project_capped_simplex(v, z=1.0, u=0.1):\n",
    "    # 1) sort v in descending order\n",
    "    u_sorted, _ = torch.sort(v, descending=True)\n",
    "    # 2) find the threshold theta for the simplex proj\n",
    "    cssv = torch.cumsum(u_sorted, dim=0) - z\n",
    "    ind = torch.arange(1, len(v)+1, device=v.device, dtype=v.dtype)\n",
    "    cond = u_sorted - cssv/ind > 0\n",
    "    rho = ind[cond][-1]\n",
    "    theta = cssv[cond][-1] / rho\n",
    "    # 3) project onto simplex\n",
    "    w = torch.clamp(v - theta, min=0.0)\n",
    "    # 4) enforce box constraint\n",
    "    w = torch.clamp(w, max=u)\n",
    "    # 5) re-project to simplex if sum ≠ z\n",
    "    if abs(w.sum().item() - z) > 1e-6:\n",
    "        # another round of simplex projection:\n",
    "        w_sorted, _ = torch.sort(w, descending=True)\n",
    "        cssv2 = torch.cumsum(w_sorted, dim=0) - z\n",
    "        cond2 = w_sorted - cssv2/ind > 0\n",
    "        rho2 = ind[cond2][-1]\n",
    "        theta2 = cssv2[cond2][-1] / rho2\n",
    "        w = torch.clamp(w - theta2, min=0.0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.FloatTensor is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 5. (Optional) Control number of CPU threads if on CPU\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# torch.set_num_threads(8)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m compiled_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n",
      "File \u001b[0;32m~/anaconda3/envs/stock/lib/python3.11/site-packages/torch/nn/modules/module.py:635\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Add a child module to the current module.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, Module) \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 635\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(module)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a Module subclass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule name should be a string. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.FloatTensor is not a Module subclass"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "# 1. Prepare data: mu (expected returns) and sigma (std deviations)\n",
    "#    Suppose you already have two 1D NumPy arrays of length N=4000:\n",
    "#      mu_np    = np.array([...], dtype=np.float32)\n",
    "#      sigma_np = np.array([...], dtype=np.float32)\n",
    "#    Here we just illustrate with random data.\n",
    "N = all_mu.shape[0]\n",
    "mu    = torch.tensor(all_mu, dtype=torch.float32)  # e.g. expected returns\n",
    "sigma = torch.tensor(S.values, dtype=torch.float32)  # e.g. covariance matrix\n",
    "\n",
    "# 2. Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mu    = mu.to(device)\n",
    "sigma = sigma.to(device)\n",
    "\n",
    "# 3. Initialize weights (unconstrained parameter)\n",
    "initial_guess = N * [1. / N]\n",
    "raw_w = torch.tensor(initial_guess, dtype=torch.float32, device=device)\n",
    "raw_w = torch.nn.Parameter(raw_w)\n",
    "\n",
    "# 4. Set up optimizer\n",
    "optimizer = torch.optim.Adam([raw_w], lr=1e-4)\n",
    "\n",
    "# 5. (Optional) Control number of CPU threads if on CPU\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "compiled_model = torch.compile(model)\n",
    "# 6. Optimization loop\n",
    "for step in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    # a) apply box constraints [0, 0.1]\n",
    "    w = project_capped_simplex(raw_w)\n",
    "    #w_clamped = torch.clamp(raw_w, min=0.0, max=0.1)\n",
    "    # b) enforce sum-to-one\n",
    "\n",
    "    #w = w_clamped / (w_clamped.sum() + 1e-12)\n",
    "\n",
    "    # c) compute portfolio return and risk\n",
    "    port_ret  = torch.dot(w, mu)\n",
    "    port_var = w @ sigma @ w\n",
    "    port_risk = torch.sqrt(port_var + 1e-12)\n",
    "\n",
    "    # d) Sharpe ratio (no risk-free rate assumed)\n",
    "    #sharpe = port_ret / port_risk\n",
    "    one_sigma = port_ret - port_risk\n",
    "    # e) maximize Sharpe → minimize negative Sharpe\n",
    "    #loss = -sharpe\n",
    "    loss = -one_sigma\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # (Optional) print progress\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step:4d}  one_sigma = {one_sigma.item():.4f}\")\n",
    "\n",
    "# 7. Extract optimized weights on CPU\n",
    "w_opt = (torch.clamp(raw_w, 0.0, 0.1) / torch.clamp(raw_w, 0.0, 0.1).sum()).detach()\n",
    "w_opt = w_opt.cpu().numpy()\n",
    "# print non-zero weights\n",
    "non_zero_weights = np.where(w_opt > 0)[0]\n",
    "print(f'Number of non-zero weights: {len(non_zero_weights)}')\n",
    "print(f'Number of zero weights: {N - len(non_zero_weights)}')\n",
    "print(f'Non-zero weights: {w_opt[non_zero_weights]}')\n",
    "print(f'final return: {np.dot(w_opt, all_mu)}')\n",
    "print(f'final risk: {np.sqrt(w_opt @ S.values @ w_opt)}')\n",
    "\n",
    "# Now w_opt is your final weight vector of length 4000, summing to 1 with each ≤ 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log return 0.6259529384490016\n",
      "volatility 0.2673337869401924\n"
     ]
    }
   ],
   "source": [
    "def portfolio_volatility_log_return(weights, covariance):\n",
    "    return np.sqrt(np.dot(weights.T, np.dot(covariance, weights)))\n",
    "\n",
    "def portfolio_log_return(weights, returns):\n",
    "    return np.sum(returns*weights)\n",
    "\n",
    "print(\"log return\",portfolio_log_return(w_opt, all_mu))\n",
    "print(\"volatility\", portfolio_volatility_log_return(w_opt, S.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def get_bounds(N, lower_bound, upper_bound):\n",
    "  # for ETF, the allowed weight is between 0 and 20%\n",
    "  # for stocks, the allowed weight is between 0 and 10%\n",
    "  num_assets = N\n",
    "  if num_assets == 0:\n",
    "    return None\n",
    "\n",
    "  bounds = tuple((lower_bound, upper_bound) for asset in range(num_assets))\n",
    "  return bounds\n",
    "\n",
    "def min_func_one_sigma(weights, returns, covariance, risk_free_rate):\n",
    "  portfolio_ret = portfolio_log_return(weights, returns)\n",
    "  portfolio_vol = portfolio_volatility_log_return(weights, covariance)\n",
    "  return -(portfolio_ret - risk_free_rate - portfolio_vol)\n",
    "\n",
    "\n",
    "def min_func_sharpe(weights, returns, covariance, risk_free_rate):\n",
    "    portfolio_ret = portfolio_log_return(weights, returns)\n",
    "    portfolio_vol = portfolio_volatility_log_return(weights, covariance)\n",
    "    sharpe_ratio = (portfolio_ret - risk_free_rate) / portfolio_vol\n",
    "    return -sharpe_ratio # Negate Sharpe ratio because we minimize the function\n",
    "\n",
    "def optimize_portfolio(returns, covariance, risk_free_rate, bounds):\n",
    "    num_assets = len(returns)\n",
    "    args = (returns, covariance, risk_free_rate)\n",
    "\n",
    "    # Define constraints\n",
    "    def constraint_sum(weights):\n",
    "        return np.sum(np.abs(weights)) - 1\n",
    "\n",
    "    constraints = [{'type': 'eq', 'fun': constraint_sum}]\n",
    "\n",
    "\n",
    "    # Perform optimization\n",
    "    def objective(weights):\n",
    "        return min_func_one_sigma(weights, returns, covariance, risk_free_rate)\n",
    "\n",
    "    iteration = [0]  # mutable container to store iteration count\n",
    "    def callback(weights):\n",
    "        iteration[0] += 1\n",
    "\n",
    "        print(f\"Iteration: {iteration[0]}, value: {objective(weights)}\")\n",
    "\n",
    "    # Initial guess (equal weights)\n",
    "    initial_guess = num_assets * [1. / num_assets]\n",
    "\n",
    "    # Perform optimization\n",
    "    result = minimize(objective, initial_guess,\n",
    "                      method='SLSQP', bounds=bounds, constraints=constraints, callback=callback, options={'maxiter': 10})\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, value: -0.3155814297010861\n",
      "Iteration: 2, value: -0.3449851843670154\n",
      "Iteration: 3, value: -0.3560713526037627\n",
      "Iteration: 4, value: -0.3577201986370235\n",
      "Iteration: 5, value: -0.358071357987287\n",
      "Iteration: 6, value: -0.3582133781268702\n",
      "Iteration: 7, value: -0.3583134038206965\n",
      "Iteration: 8, value: -0.3583271881336771\n",
      "Iteration: 9, value: -0.3583336935040428\n",
      "Iteration: 10, value: -0.3583336935040428\n",
      "final return:  0.6224007586024712\n",
      "final risk:  0.2640670650984284\n",
      "final weights:  [0.00000000e+00 0.00000000e+00 5.04896354e-15 ... 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "bounds = get_bounds(all_mu.shape[0], 0, 0.1)\n",
    "raw_weights = optimize_portfolio(all_mu, S, 0, bounds)\n",
    "print('final return: ', portfolio_log_return(raw_weights.x, all_mu))\n",
    "print('final risk: ', portfolio_volatility_log_return(raw_weights.x, S.values))\n",
    "print('final weights: ', raw_weights.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero weights: [0.02842462 0.1        0.09987548 0.0817379  0.06508591 0.1\n",
      " 0.08406259 0.0472043  0.1        0.02979073 0.06381847 0.1\n",
      " 0.1       ]\n"
     ]
    }
   ],
   "source": [
    "non_zero_weights = np.where(raw_weights.x > 0.005)[0]\n",
    "print(f'Non-zero weights: {raw_weights.x[non_zero_weights]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
