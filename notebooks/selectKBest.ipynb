{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from util import get_X_y_by_stock, get_tickers, get_currency_pair, load_latest_price_data, convert, add_features, load_pkl, merge_fred, remove_nan\n",
    "from util import save_pkl, get_pipline_svr, get_pipline_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'../processed_data_{period}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger('training')\n",
    "logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "\n",
    "df_train_X_all = load_pkl(f'{data_dir}/df_train_X_all.pkl')\n",
    "df_train_y_all = load_pkl(f'{data_dir}/df_train_y_all.pkl')\n",
    "df_test_X_all = load_pkl(f'{data_dir}/df_test_X_all.pkl')\n",
    "df_test_y_all = load_pkl(f'{data_dir}/df_test_y_all.pkl')\n",
    "with open(f'{data_dir}/valid_tickers.txt', 'r') as f:\n",
    "    valid_tickers = f.read().splitlines()\n",
    "logger.info(f'Data loaded from {data_dir}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the logger\n",
    "logger = logging.getLogger('training')\n",
    "logger.setLevel(logging.DEBUG)  # Set the logging level\n",
    "\n",
    "# Load the pickled data\n",
    "def load_pkl(path):\n",
    "    return pd.read_pickle(path)\n",
    "\n",
    "df_train_X_all = load_pkl(f'{data_dir}/df_train_X_all.pkl')\n",
    "df_train_y_all = load_pkl(f'{data_dir}/df_train_y_all.pkl')\n",
    "df_test_X_all = load_pkl(f'{data_dir}/df_test_X_all.pkl')\n",
    "df_test_y_all = load_pkl(f'{data_dir}/df_test_y_all.pkl')\n",
    "with open(f'{data_dir}/valid_tickers.txt', 'r') as f:\n",
    "    valid_tickers = f.read().splitlines()\n",
    "\n",
    "logger.info(f'Data loaded from {data_dir}...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stock_gspc(df_stock_X, df_stock_y, df_gspc):\n",
    "    # Get the later of the two start dates\n",
    "    df_combined = df_stock_X.join(df_gspc, how='left')\n",
    "    start_date = max(df_stock_X.index.min(), df_gspc.index.min())\n",
    "    # Cut the combined dataframe to start from the later start date\n",
    "    df_combined = df_combined.loc[start_date:]\n",
    "    df_stock_y = df_stock_y.loc[start_date:]\n",
    "    # Forward fill missing values in the 'predicted_gspc' column\n",
    "    df_combined['predicted_gspc'] = df_combined['predicted_gspc'].ffill()\n",
    "    # If there are still any NaN values at the beginning, you might want to back fill them\n",
    "    df_combined['predicted_gspc'] = df_combined['predicted_gspc'].bfill()\n",
    "    return df_combined, df_stock_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "feature_names = list(df_train_X_all[0].columns)\n",
    "coefficients = np.zeros(len(feature_names))\n",
    "for i in range(len(valid_tickers)):\n",
    "    ticker = valid_tickers[i]\n",
    "    df_train_X_stock = df_train_X_all[i]\n",
    "    df_train_y_stock = df_train_y_all[i]\n",
    "    # concat the gspc data to the d_train_X_stock\n",
    "\n",
    "    # scale the features\n",
    "    scaler = StandardScaler()\n",
    "    df_train_X_stock = scaler.fit_transform(df_train_X_stock)\n",
    "\n",
    "    # Apply feature selection using SelectKBest\n",
    "    #selector = SelectKBest(f_regression, k=5)  # Select 5 best features\n",
    "    # X_new = selector.fit_transform(df_train_X_stock, df_train_y_stock['log_predict'])\n",
    "\n",
    "    # Fit linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(df_train_X_stock, df_train_y_stock['log_predict'])\n",
    "\n",
    "    # Get the coefficients of the selected features:\n",
    "    coefficients += model.coef_\n",
    "\n",
    "    # Mapping back to feature names (assuming you have a list of feature names)\n",
    "    \n",
    "\n",
    "feature_impact = dict(zip(feature_names, coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Assuming selector.scores_ is the array of scores for each feature\n",
    "scores = coefficients\n",
    "\n",
    "# Get the scores in descending order\n",
    "sorted_scores = scores.argsort()[::-1]  # Sort indices in descending order of scores\n",
    "feature_names = np.array(feature_names)\n",
    "sorted_features = feature_names[sorted_scores]\n",
    "sorted_scores = scores[sorted_scores]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_features, sorted_scores, color='skyblue',width=0.4)\n",
    "plt.xlabel('Feature Names')\n",
    "plt.ylabel('Scores (Descending Order)')\n",
    "plt.title('Feature Scores Plot')\n",
    "plt.xticks(rotation=45)  # Rotate feature names for better visibility\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = coefficients\n",
    "\n",
    "# Get the scores in descending order\n",
    "sorted_scores = np.abs(scores).argsort()[::-1]  # Sort indices in descending order of scores\n",
    "feature_names = np.array(feature_names)\n",
    "sorted_features = feature_names[sorted_scores]\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sorted_features to a file\n",
    "with open(f'{data_dir}/sorted_features.txt', 'w') as f:\n",
    "    for item in sorted_features:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X_all = load_pkl(f'{data_dir}/df_train_X_all.pkl')\n",
    "df_train_y_all = load_pkl(f'{data_dir}/df_train_y_all.pkl')\n",
    "df_test_X_all = load_pkl(f'{data_dir}/df_test_X_all.pkl')\n",
    "df_test_y_all = load_pkl(f'{data_dir}/df_test_y_all.pkl')\n",
    "with open(f'{data_dir}/valid_tickers.txt', 'r') as f:\n",
    "  valid_tickers = f.read().splitlines()\n",
    "\n",
    "with open(f'{data_dir}/sorted_features.txt', 'r') as f:\n",
    "  sorted_features = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_tickers)):\n",
    "  df_train_X_stock = df_train_X_all[i]\n",
    "  df_train_y_stock = df_train_y_all[i]\n",
    "  df_test_X_stock = df_test_X_all[i]\n",
    "  df_test_y_stock = df_test_y_all[i]\n",
    "  df_train_X_stock, df_train_y_stock = combine_stock_gspc(df_train_X_stock, df_train_y_stock, df_gspc)\n",
    "  df_test_X_stock, df_test_y_stock = combine_stock_gspc(df_test_X_stock, df_test_y_stock, df_gspc)\n",
    "\n",
    "\n",
    "  df_train_X_all[i] = df_train_X_stock[sorted_features]\n",
    "  df_test_X_all[i] = df_test_X_stock[sorted_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from safeRegressors import TimeoutException\n",
    "\n",
    "def objective_random_forest(trial, valid_tickers, df_train_X_all, df_train_y_all):\n",
    "  # Define the hyperparameter configuration space\n",
    "  k = trial.suggest_int('k', 5, len(df_train_X_all[0].columns))\n",
    "  n_estimators = trial.suggest_int('n_estimators', 20, 160)\n",
    "  max_depth = trial.suggest_int('max_depth', 10, 50)\n",
    "  min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "  min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "  max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "  bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "  max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 6, 200)\n",
    "\n",
    "  params = {\"n_estimators\": n_estimators, \"max_depth\": max_depth, \\\n",
    "            \"min_samples_split\": min_samples_split, \\\n",
    "              \"min_samples_leaf\": min_samples_leaf, \"max_features\": max_features, \\\n",
    "                \"bootstrap\": bootstrap, \"max_leaf_nodes\": max_leaf_nodes, \"k\": k}\n",
    "  \n",
    "  pipeline = get_pipline_rf(params)\n",
    "\n",
    "  total_mses = 0\n",
    "  try:\n",
    "    for i in range(len(valid_tickers)):\n",
    "      if i % 100 == 0:\n",
    "        logger.info(f'Processing {i}th stock...')\n",
    "      df_train_X = df_train_X_all[i]\n",
    "      df_train_y = df_train_y_all[i]\n",
    "\n",
    "      X_train = df_train_X.copy().values\n",
    "      y_train = df_train_y.copy().values.ravel()\n",
    "      predictions = cross_val_predict(pipeline, X_train, y_train, cv=5, n_jobs=5)\n",
    "      mse = mean_squared_error(y_train, predictions)\n",
    "      total_mses += mse\n",
    "    \n",
    "    return total_mses/len(valid_tickers)\n",
    "  except TimeoutException:\n",
    "      logger.error(\"A timeout has occurred during model fitting.\")\n",
    "      # Return a large MSE value to penalize this result\n",
    "      return float('inf')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "study_rf = optuna.create_study()\n",
    "study_rf.optimize(lambda trial: objective_random_forest(trial, valid_tickers, df_train_X_all, df_train_y_all), \n",
    "                    n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Iterate through the stocks and select the best features for each\n",
    "tot_scores = None\n",
    "for i in range(len(valid_tickers)):\n",
    "    ticker = valid_tickers[i]\n",
    "    # Filter the training and testing data for the current stock\n",
    "    df_train_X_stock = df_train_X_all[i]\n",
    "    df_train_y_stock = df_train_y_all[i]\n",
    "\n",
    "    # scale the features\n",
    "    scaler = StandardScaler()\n",
    "    df_train_X_stock = scaler.fit_transform(df_train_X_stock)\n",
    "\n",
    "    # Apply feature selection using SelectKBest\n",
    "    selector = SelectKBest(f_regression, k=5)  # Select 5 best features\n",
    "    X_new = selector.fit_transform(df_train_X_stock, df_train_y_stock['log_predict'])\n",
    "\n",
    "    # Fit linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_new, df_train_y_stock['log_predict'])\n",
    "\n",
    "    scores = selector.scores_\n",
    "\n",
    "    # make element-wise addition for tot_scores\n",
    "    if tot_scores is None:\n",
    "        tot_scores = scores/len(valid_tickers)\n",
    "    else:\n",
    "        tot_scores += scores/len(valid_tickers)\n",
    "\n",
    "# Get the scores in descending order\n",
    "feature_names = df_train_X_all[0].columns\n",
    "sorted_scores = scores.argsort()[::-1]  # Sort indices in descending order of scores\n",
    "sorted_features = feature_names[sorted_scores]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores in descending order\n",
    "sorted_scores = scores.argsort()[::-1]  # Sort indices in descending order of scores\n",
    "sorted_features = df_train_X_stock.columns[sorted_scores]\n",
    "print(sorted_features)\n",
    "\n",
    "# iterate all tickers, reorder the features based on the scores by descending order\n",
    "for i in range(len(valid_tickers)):\n",
    "    ticker = valid_tickers[i]\n",
    "    df_train_X_all[i] = df_train_X_all[i][sorted_features]\n",
    "\n",
    "\n",
    "\n",
    "    # Now you have `selected_features_train` and `selected_features_test` which contain the selected features for the current stock.\n",
    "    # You can continue with training or any further processing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(valid_tickers)):\n",
    "    ticker = valid_tickers[i]\n",
    "    # Filter the training and testing data for the current stock\n",
    "    df_train_X_stock = df_train_X_all[i]\n",
    "    print(df_train_X_stock.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
